{"meta":{"title":"Machinelearner Thoughts","subtitle":"Rants on Architecture, Distributed Systems, Machine Learning, Data applications","description":"Rants on Architecture, Distributed Systems, Machine Learning, Data applications, Business models, Digital Economics, Software 2020, Value drive software delivery","author":"machinelearner","url":"machinelearner.in"},"pages":[{"title":"Categories","date":"2016-08-17T22:34:06.000Z","updated":"2016-08-17T20:37:46.000Z","comments":true,"path":"categories/index.html","permalink":"machinelearner.in/categories/index.html","excerpt":"","text":""},{"title":"About","date":"2016-08-24T12:45:25.000Z","updated":"2016-08-24T12:45:25.000Z","comments":true,"path":"about/index.html","permalink":"machinelearner.in/about/index.html","excerpt":"","text":"Machine LearningDistributed SystemsBelieves in making world a more rational placeFootballSingingMythologyTeaching the ways of learning(convoluted, yes)Believes in zero connection between experience and reasoning Currently Spending Brain Cycles at: ThoughtWorksOccupied Brain Cycles include: Economics, State of demand in Software Industry, Business models and disruption in various sectors, Intelligent ecosystems, Distributed Systems"},{"title":"","date":"2016-08-20T12:50:57.000Z","updated":"2016-08-20T12:50:57.000Z","comments":true,"path":"css/style_override.css","permalink":"machinelearner.in/css/style_override.css","excerpt":"","text":"body {text-align:justify;}"},{"title":"Tags","date":"2016-08-17T22:34:06.000Z","updated":"2016-08-17T20:37:46.000Z","comments":true,"path":"tags/index.html","permalink":"machinelearner.in/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Fladoozive - Distributed Log Processing System","slug":"fladoozive-distributed-log-processing-system","date":"2013-11-25T16:41:00.000Z","updated":"2016-08-17T22:09:39.000Z","comments":true,"path":"2013/11/25/fladoozive-distributed-log-processing-system/","link":"","permalink":"machinelearner.in/2013/11/25/fladoozive-distributed-log-processing-system/","excerpt":"Hello Hive Mind, Logs with diagnosis and system health monitoring data can be used to tune the system/process perform better. One of our clients, one of the largest manufacturers of computer hardware, approached us with an interesting problem with serious business concern. Dealing with increasing, precious log data was a problem that was knocking at their doorstep. The challenging bit for us was to design a system, which can handle the scale and also allow for building models, which can analyze the collected data.","text":"Hello Hive Mind, Logs with diagnosis and system health monitoring data can be used to tune the system/process perform better. One of our clients, one of the largest manufacturers of computer hardware, approached us with an interesting problem with serious business concern. Dealing with increasing, precious log data was a problem that was knocking at their doorstep. The challenging bit for us was to design a system, which can handle the scale and also allow for building models, which can analyze the collected data. The following is a journey of how we solved a distributed Log-Processing problem by integrating various systems built around the Hadoop Eco-System. The problem at hand was to build an efficient system, which could make querying of semi-structured data more easy. This meant that the system had to handle the ever-growing data in an efficient manner by ingesting, processing and storing data into a reliable and query-able eco-system. Hadoop is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage. Rather than relying on hardware to deliver high-availability, the framework itself is designed to detect and handle failures at the application layer and hence delivering a highly available service on top of a cluster of computers, each of which may be prone to failures. Why Hadoop?This was one of the primary questions, which our client had, Why Hadoop? There were certain amount of constraints and the nature of data itself, which pushed us to choose Hadoop and its eco-system. Volume of data – It was primarily tens of thousands of devices, manufactured and deployed by our client, sending diagnostic reports on a periodic basis. It could be any where in the range of several tens or hundreds of GBs of data per day. Apart from the scale issue, the core problem was that the diagnostic logs had a variety of formats. Currently they had an ETL process that stages the data and uploads to their final store for analysis. With Hadoop, since the file system does not impose any restrictions on the format of data stored, it will be easy to on-board the data and perform ETL on top of it from within Hadoop. This would allow the ETL process to be parallelized through MapReduce and made more efficient. Also, the data was sensitive and not to be made available outside of client infrastructure (Ruled out a lot of proprietary and cloud based solutions, the likes of Splunk, Big Query etc.) The other requirement was the ability to build statistical or otherwise models, which derived useful insights from the data. It was also required that the current online applications consuming the data (either for reporting purpose, monitoring purpose or for other processing) should still be able to function with minor changes and without much effort. Summarizing all of it, it was required to build a system, which could collect data from various sources in real-time, pre-process to get it into a query-able format and in the end extract information to more traditional storage mechanisms like RDBMS for applications reporting the derived insights. This is where Hadoop felt right at home. Reliable storage of large scale data, query using HIVE, pre-processing either using Pig or custom map-reduce jobs, complex workflow management using Oozie or Cascading or Azkaban, data ingestion using Flume or Storm or Scribe and data migration using Sqoop. It is important to understand the practices, workflows and usage patterns commonly seen in many enterprise systems while building a custom solution. The following is a summary of our understanding with BigData applications. The data lifecycle describes how data moves through the application from ingestion to archival. One framework we have found useful for analyzing the way the application needs to be built is to look at each phase of the lifecycle, consider what are our requirements, trade-offs, technological considerations and choices. Acquisition / Ingestion This stage in the application is responsible for handling the gushing stream of input data. The solution at this stage is responsible for controlling certain scale related parameters like: How fast or slow do we require data to move in? Is it to be streamed or can we move it as bulk uploads? What is the unit of processing? – Generally in a temporal scope – for e.g. daily, hourly etc. Preparation Conventional ETL (Extract, Transform, Load) phase. This is also the phase where we might want to consider how we will manage our schema. How we will manage changes to schema etc. Also decide what our requirements are for anonymizing data, how to handle missing values etc. The data that comes in from the acquisition stage is ‘raw’ data. After the preparatory stages, the data is generally ready for consumption by users of the application. However, we can look at storing the raw data in addition to processed data for archival purposes. We can control access to ‘raw’ data to administrators alone. Processing This is the phase where we run our descriptive or predictive statistical algorithms. This could also be where the data is opened up for ad-hoc or exploratory analysis.Presentation: Given the high latency nature of processing, we would like to extract commonly used details for consumption by applications that have low latency requirements. This is usually related to meeting standard reporting requirements, etc. We need to decide what our integration requirements are and how to ensure the data is available for that consumption. Archival This generally is related to compressing and offloading data, for purposes of compliance. This is important to consider as it does drive requirements for capacity planning etc. Having seen the common usage pattern, this section describes an example architecture, which points out the different set of systems and where they can fit in a BigData solution. The first step in an analytical process is generally to move from sparse data sources to a co-existing analytical store. Access patterns in an analytical processing system are different from traditional systems. The data is analyzed as a whole set and involves bulk-processing, de-normalized forms etc. and hence analytical stores have special characteristics. Stores can be big data file systems like HDFS, NoSQL databases like HBase, or enterprise traditional data warehouses. This is where data ingestion systems come into picture, which handles the movement of data into analytical stores. The Processing layer is split into two, processing and query. The processing layer performs data manipulation using some sort of ETL process or use of various statistical/machine-learning algorithms, which consume data to derive some insight. The query layer involves data arrangement and structuring so that it can be easily retrieved later. In general, this layer provides a SQL interface on top of the underlying data/analytical stores. The output of the processing layer could be put back to the stores for more efficient query in the presentation layer. The following figure shows the schematic of the various components in analytical big data solution and how they assemble together. With the understanding of generic and commonly seen architecture, we present Fladoozive and its components. How did we get the data in? One of the first problems we had to solve was to get the data into a place where we can use it for further processing. Having reasoned out to choose Hadoop’s HDFS as the analytical store, the idea was to use a component which can work in a reliable fashion but could also write to HDFS. We evaluated Scribe, a server for aggregating log data streamed in real time from a large number of servers, designed to be scalable, extensible without client-side modification, and robust to failure of the network or any specific machine. Facebook developed Scribe and they have used the same for all their log related problems. The problem we faced with Scribe was with the time consuming nature of debugging the scribe instance to write to HDFS. Without the official/community support and the less active development implies that maintaining a system built using this would have costly implication in the future if there is a need to upscale or there is a upgrade in other components. Other options available for large-scale distributed data ingestion component were Twitter Storm and Apache Flume (Couple of other proprietary and OSS implementation we haven’t discussed here). Storm is a distributed realtime computation system. Similar to how Hadoop provides a set of general primitives for doing batch processing, Storm provides a set of general primitives for doing realtime computation. The kind of data requirement we had in hand was not so inclined towards primitive realtime computation/data processing and would have seemed an overkill to use storm. The primary task of the ingest component was to provide a scalable mechanism to get data into HDFS. Apache Flume (Flume-NG is the new generation version of the original apache flume project and the we mean Flume-NG when we refer to flume) is a distributed, reliable, and available service for efficiently collecting, aggregating, and moving large amounts of log data. It has a simple and flexible architecture based on streaming data flows and provides many reliable, failover, recovery mechanism, which makes it robust and fault tolerant. Flume servers were configured to receive Avro (Data serialization mechanism) messages from different clients emitting logs. The Flume server provides certain handles, which can be used to tune the system for desired operational requirements. It has a concept of channel, which can be used as fast intermediate queues before writing to the sink. MemoryChannel provides high throughput but loses data in the event of a crash or loss of power. Relatively more persistent channel is a FileChannel. The goal of FileChannel is to provide a reliable high throughput channel. FileChannel guarantees that when a transaction is committed, no data will be lost due to a subsequent crash or loss of power. Channels have different configuration capabilities to control the velocity of flow and the latency needs before writing to HDFS or another set of flume servers (in general, sink). In a complex and massively high throughput environment the flume servers can be arranged in a layered architecture to add more reliability and throttling mechanism and hence control the data flow. Flume can write to a set of sinks HDFS, AVRO, File, HBASE, ElasticSearch etc. In our case the final Flume sink wrote to HDFS and was configured for various write characteristics like rollInterval, size, count etc. What did we do with the data? Data Organization and Processing Data life cycle involves storing data at three different stages in processing/analysis. The raw ingested data forms the initial stage in the lifecycle where it is primarily stored for further analysis. This stage may involve creation of suitable directory structure, which would help in making temporal decision either for computational purposes, compliance or monitoring purposes. The next stage is the processing/processed stage where the data after some processing is put in a directory structure, which is more suitable for the usage pattern. Incase the data is used to get timely reports/insights, and uses a Hadoop based querying solution then it’s a good idea to partition the data which will aid in modeling this better. The third stage is the archival stage where the data is stored in a compressed format for any further requirements, be it legal or verification etc. In the system we developed, the lowest level of directory had all the logs for a given day. The logs themselves did not have enough (rather any) meta data to suggest they should goto a certain directory. This is where Flume’s source interceptors came to rescue. TimeStamp interceptor was used to first tag the incoming log and use this tag information to create a directory on HDFS. This facility/feature was provided by Flume’s HDFS sink dynamic path, which had the ability to parse interceptor/header information. Flume ingested data formed the raw stage of the lifecycle. Processing stage involved running mapreduce Jobs to perform the data cleansing and preparation or in the BI world it’s equivalent to performing ETL (Extract Transform and Load) operations. The resultant data was written on to a processed section with a similar directory structure as the raw data. The important factor here was that the chosen directory structure aided in creating Hive partitions for querying which we will talk about in the next sub-section. Now that there is data on HDFS, How do you make it query-able? Hive is a data warehouse system for Hadoop that facilitates easy data summarization, ad-hoc queries, and the analysis of large datasets stored on HDFS or the kinds. It provides a SQL like querying mechanism called Hive Query Language (HQL), which in turn is converted to a set of map reduce jobs and run by Hive to fetch the results. Hive maintains Hive table meta information in an external database (can be configured to point to Mysql databse, and we used external MySql db as the metastore). The challenge was to make Hive point to the newly available data as and when they are available after processing. This was taken care by adding the processed directory, as a new Hive partition after the processing map-reduce job was complete. Hive partitions work in a similar fashion as that of a RDBMS partition. Hive partitions can point to directories on HDFS and the directory structure can act as column partition values. Adding a partition implies a new entry in the Hive metastore and thus the new data can be made available for querying in Hive. Hive provides a sql like interface which can be used to perform exploratory analysis and run queries. Querying capability, check ✓ Now, How do I make this analyzed data available to my traditional reporting application accessing information from RDBMS? Exporting data from HDFS to External RDBMS is a common problem faced by Analytical stores and solutions. We have used Sqoop, a tool designed for efficiently transferring bulk data between Apache Hadoop and structured datastores such as relational databases. Sqoop can work with Mysql out of the box using the JDBC driver to connect to it. It also has support to take in a custom driver and connect to other proprietary RDBMS e.g. Microsoft’s SQLServer. In Fladoozive, the output of a Hive query was written on to HDFS and Sqoop was used to pick up this result and populate a remote RDBMS table. In this way, a system, which reports insights on a timely manner can still work on the traditional RDBMS without having to move to a distributed paradigm as a set of operation can perform computation in batches on a distributed eco-system like Hadoop and dump the final output to RDBMS. Talking about automation and integration of so many systems, How can I manage these workflows using just one tool/framework? The typical process, which we emphasized in the previous sections, involves performing tasks in an orderly fashion, as a workflow. There are a bunch of options available to manage Hadoop related workflows e.g. Cascading, Oozie, Azkaban, Hamake etc. In our case the requirement was to be able to include actions from different components of Hadoop Ecosystem as part of one workflow. Cascading provides Java apis to schedule several kinds of jobs, Azkaban and Hamake have other ways to express a workflow but on the other hand support very few kinds of Hadoop related jobs. Given our requirement to integrate Mapreduce, Hive, Sqoop actions in a workflow, Oozie seemed a better fit as it supported most of these jobs out of the box. Oozie is a workflow scheduler system built to manage Hadoop Jobs. It is integrated with the rest of the Hadoop stack supporting several types of Hadoop jobs out of the box (such as Java map-reduce, Streaming map-reduce, Pig, Hive, Sqoop and Distcp) as well as system specific jobs (such as Java programs and shell scripts). Once Oozie is configured to work with Hive, Sqoop it can perform a task pertaining to these tools. Configuring Oozie to work with them at times simply involves making the java package jars available in the execution path on HDFS. Oozie workflow can simplify the complexity involved in liking multiple Hadoop Map Reduce jobs and also can aid in the easy execution of other tools in the Hadoop Ecosystem which come as part of a particular data work flow. We had two sets of Oozie workflow: Pure ETL and processing, which would transform data into query-able format E.g. add Hive partition after some processing. Jobs of query and export nature. Building the system gave us opportunity to understand better the distributed computing paradigm at an enterprise scale and what it takes to deal with the obstacles encountered along the way. ConclusionsIt was satisfying to see a system getting built over the course of a week and a half and just work. All of our choices were Open Source applications and for organizations looking for a custom solution due to privacy requirements or extensibility of use, at lower cost and wide community support, we would definitely recommend use of Hadoop Eco-System. There are certain areas in which the whole eco-system can improve. There are already signs that the community is doing everything there is to get to that stage.There is a solution available for any specific problem, which one can think of. The ecosystem is evolving big and strong, there are already systems adopting newer features in Hadoop (YARN) and leveraging this to build more sophisticated systems which could work on the Hadoop stack. One of such discrete example is the evolution of SQL like solution that use HDFS. It is not long that Hadoop and its Eco-System becomes the De-Facto Standard for any analytical/Big Data solution. Highlighting Important Lessons Learnt Always check for version compatibility before proceeding: When integrating different components, always check if the versions of the components can co-exist. Tiny peril of using OSS components but on the contrary, public support forums come to the rescue in no time. Be prepared to get your hands dirty with the source code if necessary (which is rare) the component is in its very nascent stage and there is relatively less documentation around certain not fully evolved feature. [Originally Posted on Blogspot]","categories":[{"name":"tech","slug":"tech","permalink":"machinelearner.in/categories/tech/"},{"name":"distributed","slug":"tech/distributed","permalink":"machinelearner.in/categories/tech/distributed/"},{"name":"application","slug":"tech/distributed/application","permalink":"machinelearner.in/categories/tech/distributed/application/"}],"tags":[{"name":"Flume","slug":"Flume","permalink":"machinelearner.in/tags/Flume/"},{"name":"Hadoop","slug":"Hadoop","permalink":"machinelearner.in/tags/Hadoop/"},{"name":"Hive","slug":"Hive","permalink":"machinelearner.in/tags/Hive/"},{"name":"Log Processing","slug":"Log-Processing","permalink":"machinelearner.in/tags/Log-Processing/"},{"name":"Oozie","slug":"Oozie","permalink":"machinelearner.in/tags/Oozie/"},{"name":"Sqoop","slug":"Sqoop","permalink":"machinelearner.in/tags/Sqoop/"}],"keywords":[{"name":"tech","slug":"tech","permalink":"machinelearner.in/categories/tech/"},{"name":"distributed","slug":"tech/distributed","permalink":"machinelearner.in/categories/tech/distributed/"},{"name":"application","slug":"tech/distributed/application","permalink":"machinelearner.in/categories/tech/distributed/application/"}]},{"title":"The Bubble around NLP and Article Summarization. A text summarization engine!!","slug":"the-bubble-around-nlp-and-article-summarization-a-text-summarization-engine","date":"2013-11-25T05:01:21.000Z","updated":"2016-08-17T22:17:29.000Z","comments":true,"path":"2013/11/25/the-bubble-around-nlp-and-article-summarization-a-text-summarization-engine/","link":"","permalink":"machinelearner.in/2013/11/25/the-bubble-around-nlp-and-article-summarization-a-text-summarization-engine/","excerpt":"Hello Hive Mind, The development of the following piece was mainly during the same time as the acquisition of Summly by Yahoo!. The main intention in developing such a solution was to generate a stream summarization where a part of the article is edited by many users and as a article owner one would want to maintain a master version which would consist of the best of all the edits. The description of the thought-process while solving such a problem is the main content of this article.","text":"Hello Hive Mind, The development of the following piece was mainly during the same time as the acquisition of Summly by Yahoo!. The main intention in developing such a solution was to generate a stream summarization where a part of the article is edited by many users and as a article owner one would want to maintain a master version which would consist of the best of all the edits. The description of the thought-process while solving such a problem is the main content of this article. Questions asked Can the subject document be summarized to highlight the most relevant points? Can the comments/edits to particular sections be summarized to capture the essence of all the edits to that section? How was this solved? Text summarization, typical text-processing problem, is extensively used in search engine optimization (but for a different purpose). I’ve created, structured and tailored this to summarize documents to improve user experience and summarize comments/edits as a mechanism for information enhancement. Article Summarization 2 broad ways of solving this: 1) Extract 2) AbstractExtract is the process of extracting sentences from the main document which can capture the essence of the document without modifying the sentence structure or creating new text in coming up with a summary.Abstract is the process of generating document summary by synthesizing text by capturing the intent, emotion etc. etc. of the author in a particular context (Slightly harder problem to crack). This system goes the extract way to summarize documents/comments/edits. It is a simple weighting and relevance based extraction, which identifies sentences that are important in the document context and includes them to be part of the summary. Two parts for extraction Weighting Measure The idea is to weight sentences based of things like where is the sentences positioned in the document, what are the kinds of words which are present in the document, are there named entities in the sentences E.g. Name of a person, organization, location etc. Theory behind this Sentences weight based on position is because of the general notion of providing information upfront and slightly fading away towards the end(Writers call it the inverted pyramid style of writing). The measure again is to avoid not so informative sentences, which tend to fall in the middle of the argument. But this does not imply these sentences are not included at all. It depends on the other two weighting criteria as well. The second parameter is to identify sentences, which consists of words that are important to a particular collection of documents/document E.g. If the document set is world politics, then the words belonging to world-genre and politics would have more weightage than the words belonging to lets say music. One such measure is TFIDF – Term Frequency Inverse Document Frequency, which is calculated for a category of documents, which represents the importance of a word/gram to the given category. Nouns, proper nouns in particular give out more information individually than other kind of words. The third parameter is used to capture this and include this as part of the weighting measure. Name Entity Recognition is a NLP process, which allows one to extract named entities in a give piece of text. Weight of Sentence i = alpha * X(i) + beta * Y(i) + gamma * Z(i) Where, Alpha, beta and gamma are bias terms (co-efficients) to keep the weighting measure tunable X(i) = position based weight for sentence i Y(i) = sigma k=0 to K( normalized tfidf(k))/K K-number of terms in the sentence Z(i) = normalized NER weight Cosine Similarity Based Relevance Measure Cosine Similarity is a measure of closeness of any two given vector in N dimensional space. The value is the cosine of the angle between vectors. The idea is to represent the document and the sentences within the document as vectors mapping to the feature space of terms/grams present in the sentence.Algorithm The following steps are performed to find the summary sentences combining the above-described measures: Feature space = all terms/grams in the document (after preprocessing) Step 0:find weights for all the sentences, Step 1:represent the document and sentences as vectors in the given Feature space Step 2:find the cosine similarity of sentences with the document Step 3:select the sentence x with highest effective weight and add it to summary list where Effective weight = cos-measure of sentence i from step 2 + corresponding weight of the sentence i from step 0 Step 4: remove all the terms/grams from the feature space which were part of the chosen sentence x Step 5: check if the number of sentences in the summary list meets the compression ratio. If no then goto Step 1 Why combination of two techniques? Initially when I started, it was just the weighting measure. Then I tried the cosine similarity measure which measure the relevance better than the weighting measure. But in the end the summary should consist of both the relevance measure and also the informative measure where we are interested in what the sentences themselves have to say.(sort of incremental addition) Why step 4 in the algorithm? At the end of Step 3, the sentence obtained is the one that relates to the document the most. Selecting sentences based on above measure ensures that the summary covers the major topics and the topics, which are informative. Whereas eliminating all the terms/grams contained in x from the document in Step 4 ensures that the subsequent sentence selection will pick the sentences with a minimum overlap with x. This leads to the creation of a summary that contains little redundancy. Edit Stream Summarization Defining this problem Space Why summarize edit stream? Various applications News websites, websites with text content and lot of people talking on a particular topic/document/article/ page (100s and thousands of comments) usually, face this scenario of having to go through all comments to understand user intent. It is very likely that people are talking about the given topic/article and specifically to some part of the topic. The idea is to go through this comment stream, identify the sub-topic being discussed and group comments based on such sub-topics and give an essence what the crowd is talking about and what is the gist of what they are talking about. Information enhancement, where in the information/content provider would like to benefit from the information in comments/edits to enhance the primary/base content (in other cases, probably more of a mechanism to aggregate feedback). The capability to edit a particular document, to start with, is limited to at the paragraph level. The following are the steps involved in coming up with an edit stream summary: For Every edited paragraph do: Step 1: Cluster similar sentences in the edits for a paragraph to capture the essence of themes of edit paragraphs. The feature space is something which could be tuned accordingly looking at the amount of data being handled (keeping in mind the dimensionality aspects, convergence etc. etc. Step 2: Pick dissimilar sentences within the cluster and summarize/find relevant sentences using the weights measure at the cluster level. The extent of dissimilarity can be found using simple measures like Jaccard Index, cosine similarity, pearson co-relation coefficient etc. Step 3: Combine to represent summary of an edited paragraph. The edits and the corresponding edit summary for the first paragraph of the article shown in the previous section is as shown above. Clustering mechanism employed is called EM Clustering. Basis: Expectation Maximization Algorithm Reason why clustering works? Modeling sentences into a feature space tends to follow normal distribution (Gaussian Distribution) where many of the points tend to collect together (Measures of central tendency follow). In the sentence collection scenario (global population), there exist many such gaussians simultaneously. They can hence be modeled as mixture of one or more gaussians (Hence the name mixture model). Its hard to keep the explanation purely verbose, shoot questions if you do not understand anything or you want more explanation, the following have useful information which might help http://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model http://www.quora.com/Machine-Learning/What-is-an-intuitive-explanation-of-Gaussian-Process-Models http://stackoverflow.com/questions/11808074/what-is-an-intuitive-explanation-of-expectation-maximization-technique Explanation for EM for Gaussian Mixture Models(one of the best n simple explanation i’ve found after a lot of searching) The article summarization can be achieved using the Apache Lucene’s snippet summary capability(and built in NLP capabilities) by indexing docs and generating term frequency list and querying sentences consisting top words in the generated list against a particular document. The retrieved list of sentences can be part of summary. But this does not give the capability to tune the summarization process to tailor different kinds of documents and writing style(Its always a relevance based summary). Tech Stack Highlight: Django NLTK What more can be done? Modeling the hypothesis of Rhetorical Structure theory Use of more sophisticated NLP techniques like use of sysnets, relationship extraction etc. Until Next Time, Live Long and Prosper! [Originally Posted on Blogspot]","categories":[{"name":"tech","slug":"tech","permalink":"machinelearner.in/categories/tech/"},{"name":"analysis","slug":"tech/analysis","permalink":"machinelearner.in/categories/tech/analysis/"},{"name":"text","slug":"tech/analysis/text","permalink":"machinelearner.in/categories/tech/analysis/text/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"machinelearner.in/tags/NLP/"},{"name":"Text Processing","slug":"Text-Processing","permalink":"machinelearner.in/tags/Text-Processing/"},{"name":"Text Summarization","slug":"Text-Summarization","permalink":"machinelearner.in/tags/Text-Summarization/"}],"keywords":[{"name":"tech","slug":"tech","permalink":"machinelearner.in/categories/tech/"},{"name":"analysis","slug":"tech/analysis","permalink":"machinelearner.in/categories/tech/analysis/"},{"name":"text","slug":"tech/analysis/text","permalink":"machinelearner.in/categories/tech/analysis/text/"}]},{"title":"Sagacity - Text Sentiment Analyzer","slug":"sagacity-text-sentiment-analyzer","date":"2013-11-24T18:11:31.000Z","updated":"2016-08-17T21:51:29.000Z","comments":true,"path":"2013/11/24/sagacity-text-sentiment-analyzer/","link":"","permalink":"machinelearner.in/2013/11/24/sagacity-text-sentiment-analyzer/","excerpt":"Hello Hive-Mind, This following is a piece of work which i tried out in my free time a while ago.The emphasis was on extracting sentiment from a piece of text, preferably a tweet(and all the twittery characters follow!!). I’ve have tried solving this as a text classification problem. The system is trained using nearly a million positively and negatively labelled tweets using the LibLinear version of SVM. I’ve tried to summarize the whole process in the following sections.","text":"Hello Hive-Mind, This following is a piece of work which i tried out in my free time a while ago.The emphasis was on extracting sentiment from a piece of text, preferably a tweet(and all the twittery characters follow!!). I’ve have tried solving this as a text classification problem. The system is trained using nearly a million positively and negatively labelled tweets using the LibLinear version of SVM. I’ve tried to summarize the whole process in the following sections. Support Vector Machine: An IntroductionVisual and detailed explanation with different examples can be found in this blog article and in this paper/article. But I’ve given it a try, explaining Basic working of SVM in this section. Basic SVM algorithm is an efficient binary classifier. The idea behind SVM approach to text classification is that the text(terms/grams in text) is mapped to a feature space in N dimension. This feature space is the basis for the SVM algorithm, which determines a linear decision surface (hyperplane) using the set of labeled data within it. This surface is then used to classify future instances of data. Data is classified based upon which side of the decision surfaces it falls. SVM is applicable to both linearly separable and non-linearly separable patterns. Patterns not linearly separable are transformed using kernel functions- a mapping function, into linearly separable ones. It can be formulated as follows,The optimal hyperplane separating the two classes can be represented as: ω.X +β = 0 (1) where, X – sample input vectors defined as {(x1,y1),(x2,y2)…………(xk,yk)} xk ∈ Rn yi ∈ {1,-1} ω,β - non zero constants ω indicating the weight component and β indicating the bias component The ordered pair x,y is the representation of each input used to form hyperplane which are N dimensional vectors labeled with corresponding y. ω.X +β ≥ 1 if yi =1 (2) ω.X +β ≤ -1 if yi=-1 (3) These can be combined into one set of inequalities: yi(xi. ω + β) ≥ 1 ∀i (4) The above inequalities hold for all input samples (linearly separable and suffice the optimal hyperplane equation). The optimal hyperplane is the unique one, which separates the training data with a maximal margin. The figure 1 depicts the above mathematical representation. Figure 1: Classification in Linearly separable two class data In our experiments and the proposed system, LIBLINEAR - A Library for Large Linear Classification is used as the library providing the SVM algorithm. Feature SpaceThere are various ways of handling text-processing problems in a vector space model. The system considers a bag of words, Unigram scenario where in every word in the corpus is represented as a dimension, which make up to the feature space. There are plenty of ways to weight such features: Presence, where in the term is accounted for in a binary fashion depending on whether or not the word is present in the document or not. TFIDF – Term Frequency &amp; Inverse document frequency, where in the belonging and importance of a particular word to a particular category is accounted. TFIDF is a numerical statistic, which reflects how important a word is to a document corpus TFIDF = norm(TF) * IDF Where, TF = number of times a term occurred in the set IDF = log(|D|/||Dt|) Delta-TFIDF - This measure works best for binary classification and especially well in a sentiment analysis scenario. This kind of term frequency transformation boosts the importance of words that are unevenly distributed between the positive and negative classes and discounts evenly distributed words. This better represents their true importance within the document for sentiment classiﬁcation. The value of an evenly distributed feature is zero. The more uneven the distribution the more important a feature should be. Features that are more prominent in the negative training set than the positive training set will have a positive score, and features that are more prominent in the positive training set than the negative training set will have a negative score. This makes a clean linear division between positive sentiment features and negative sentiment features(the reason why I have chosen to use this weighting mechanism).Table 1 summarizes the top 10 words representing the positive and negative sentiment. ΔTFIDF = TFIDF+ve – TFIDF-ve Positive Sentiment Words Negative Sentiment Words thanks sucks love want good sorry great sick happy hate you wish awesome bad haha work nice miss lol sad —————————————————- Table 1: Top words occurring in positive and negative sentiments. The system can be divided into the following components: Pre-Processor This stage includes data cleaning processes required for text analysis. Cleansing is essential in any for of text analysis. The reason being the dimensionality factor. The more the number of unigrams/terms imply the dimensionality increase, which in turn result in complex computational needs and in the end a complex model. In order to avoid such a scenario, pre-processing techniques can help reduce the size of the feature space. There are other more sophisticated dimensionality reduction techniques, which can be employed, but eliminating few of the obvious things and improving unigram indicators is the easiest and the primary step. In this regard, the pre-processing comprises of removal of stop words, which are usually articles, pronouns, helping verbs etc., which are not indicative of any category. Spell-correction reduces the chances of same word being accounted twice in because they were spelt differently. This process also deals with accounting over expressiveness prior to correction. As part of pre-processing, the document/tweet is modeled into vectors and into a format such that the LibLinear classifier can easily consume these vectors. Training Training process consists of first generating the delta-tfidf dictionary which can then be used to weight vectors. The liblinear train involves choosing the right dataset and optimal training parameters for model generation. The system uses L1R_L2LOSS_SVC solver type(Tiny peek into what is L1R_L2LOSS_SVC). The output of the training process is a model, which represents the trained knowledge and is used for classification of new instances during the prediction phase. Classification This is the stage where, given a sentence, transform it to a vector and predict against the model generated during the training stage. The code for all the above process can be found at Sagacity. Will try and make the app accessible to all of you. The tech stack included the following,Ruby back end(training process)Rails front end(very simple)Ruby version of LibLinear for training and classificationMongo Analysis indicated a cross-validated(10 fold) accuracy of 78.4667% and average accuracy of ~83% with similar F-Measure values across various datasets. Lot of scope for improvement. Hopefully I’ll be able to take it forward. This is the first of many article to come. Until Next Time, Live Long and Prosper! [Originally Posted on Blogspot]","categories":[{"name":"tech","slug":"tech","permalink":"machinelearner.in/categories/tech/"},{"name":"analysis","slug":"tech/analysis","permalink":"machinelearner.in/categories/tech/analysis/"},{"name":"text","slug":"tech/analysis/text","permalink":"machinelearner.in/categories/tech/analysis/text/"}],"tags":[{"name":"Mongo","slug":"Mongo","permalink":"machinelearner.in/tags/Mongo/"},{"name":"Rails","slug":"Rails","permalink":"machinelearner.in/tags/Rails/"},{"name":"Sentiment Analysis","slug":"Sentiment-Analysis","permalink":"machinelearner.in/tags/Sentiment-Analysis/"},{"name":"Support Vector Machine","slug":"Support-Vector-Machine","permalink":"machinelearner.in/tags/Support-Vector-Machine/"}],"keywords":[{"name":"tech","slug":"tech","permalink":"machinelearner.in/categories/tech/"},{"name":"analysis","slug":"tech/analysis","permalink":"machinelearner.in/categories/tech/analysis/"},{"name":"text","slug":"tech/analysis/text","permalink":"machinelearner.in/categories/tech/analysis/text/"}]}]}